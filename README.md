
# AI Safety Assurance Demo

This repository demonstrates practical applications of AI Safety Assurance principles, aligned with:
- **AMLAS** (Assurance of Machine Learning for Autonomous Systems)
- **IEEE Systematic Literature Review (2022) on AI Safety Assurance**
- **ACM Taxonomy of Machine Learning Safety (2022)**
- **Explainable AI (XAI)** using PeBEx and SHAP
- **Policy Modeling considerations** for AI systems in the context of public policy and national security

## Structure

- `notebooks/` - Jupyter notebooks demonstrating AMLAS stages, Explainability techniques, and Safety Checklist
- `data/` - Sample data and models
- `scripts/` - Automation scripts for black-box testing, safety envelopes, and runtime error detection
- `aml_safety_argument_pattern.md` - AMLAS safety argument patterns in markdown

## Relevance to AI Policy and Governance

AI Safety Assurance is critical to responsible adoption of AI within national security and intelligence communities. This demo aligns with needs articulated in NIST AI RMF and the ODNI Responsible AI framework.

## References

- AMLAS (2021)
- IEEE SLR on AI Safety Assurance (2022)
- ACM ML Safety Taxonomy (2022)
- Latest Explainable AI approaches (2025)
- Policy Modeling with AI (2024)
